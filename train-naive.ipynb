{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10a005b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains the naive fine-tuning approach for training coarse-grained and fine-grained datasets in a continual learning classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67295c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds for reproducibility\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# seeds\n",
    "seed = 88\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb8a2b",
   "metadata": {},
   "source": [
    "## Data Loading and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# remove copyright banner\n",
    "class RemoveCopyrightBanner(object):\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        return img.crop((0, 0, width, height - 20))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    RemoveCopyrightBanner(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# granularity = 'variant'\n",
    "\n",
    "# # Create the FGVC Aircraft dataset instance\n",
    "# train_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='trainval',              # Options: 'train', 'val', 'trainval', 'test'\n",
    "#     annotation_level=granularity,    # Options: 'variant', 'family', 'manufacturer'\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# val_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='val',\n",
    "#     annotation_level='variant',\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# test_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='test',\n",
    "#     annotation_level=granularity,\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "data_root = './data'\n",
    "\n",
    "train_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='val',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainval_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [0.8, 0.2], generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a50ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show images\n",
    "def show_images(train_dataset, num_images=5):\n",
    "  #shuffle the dataset\n",
    "  train_dataset = torch.utils.data.Subset(train_dataset, torch.randperm(len(train_dataset)))\n",
    "  fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "  for i in range(num_images):\n",
    "      image, label = train_dataset[i]\n",
    "      image = image.permute(1, 2, 0)  # convert from CxHxW to HxWxC\n",
    "      axes[i].imshow(image)\n",
    "      axes[i].set_title(f'Label: {label}')\n",
    "      axes[i].axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_images(train_dataset, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85998420",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "In a continual learning setting, each task contains a new set of classes to train the model on. The validation and test datasets should be cumulative (to evalute the performance of the model on all classes seen by the model so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def group_task_indices(dataset, cumulative=False, max_per_class=1000, classes_per_task=10):\n",
    "    \"\"\"\n",
    "    Task 0: 0-9, Task 1: 10-19, ..., Task 9: 90-99 for train\n",
    "    Output a dictionary where keys are task indices and values are lists of image indices.\n",
    "    For example, task_dict[0] will contain indices of images with labels 0-9.\n",
    "    \"\"\"\n",
    "    per_class_counts = defaultdict(int)\n",
    "    task_dict = defaultdict(list)\n",
    "    for idx, (_, label) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        # for test and val, should have cumulative indices (all classes seen so far)\n",
    "        if per_class_counts[label] >= max_per_class:\n",
    "          continue\n",
    "        per_class_counts[label] += 1\n",
    "        if cumulative:\n",
    "          for i in range((label // classes_per_task), classes_per_task):\n",
    "            task_dict[i].append(idx)\n",
    "        else:\n",
    "          task_dict[label // classes_per_task].append(idx)\n",
    "    return task_dict\n",
    "\n",
    "train_task_idxs = group_task_indices(train_dataset, cumulative=False, max_per_class=60)\n",
    "val_task_idxs = group_task_indices(val_dataset, cumulative=True)\n",
    "test_task_idxs = group_task_indices(test_dataset, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b183930",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_net(net_to_val, val_loader):\n",
    "    net_to_val.eval()\n",
    "    loss = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(val_loader, desc=\"Validating\"):\n",
    "\n",
    "            # Get the input images and their corresponding labels\n",
    "            img, label = img.cuda(), label.cuda()\n",
    "\n",
    "            # Forward pass: Get predictions from the model\n",
    "            outputs = net_to_val(img)\n",
    "            loss += criterion(outputs, label)\n",
    "\n",
    "        return loss / len(val_loader)\n",
    "\n",
    "def train_net(max_epochs, freeze_epochs, patience, net_to_train, opt, train_loader, val_loader, task, save_file=None, save_path=None):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    net_to_train.cuda()\n",
    "\n",
    "    initial_freeze = (task == 0) # only freeze backbone for task 0\n",
    "\n",
    "    for name, param in net_to_train.named_parameters():\n",
    "        # do not train non-fc layers\n",
    "        if initial_freeze and 'fc' not in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = opt\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"Starting training for Task {task}. Trainable parameters:\")\n",
    "    for name, param in net_to_train.named_parameters():\n",
    "        if param.requires_grad:\n",
    "             print(f\"  - {name}\")\n",
    "\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        net_to_train.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Unfreeze logic (if desired for staged training within a task)\n",
    "        if epoch == freeze_epochs and task > 0: # Only unfreeze/adjust LR if NOT task 0 and freeze_epochs > 0\n",
    "            print(f\"Unfreezing backbone at epoch {epoch} for task {task}\")\n",
    "            for param in net_to_train.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Adjust LR for the existing optimizer\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            new_lr = 1e-4\n",
    "            if current_lr != new_lr:\n",
    "                 print(f\"Setting LR to {new_lr}\")\n",
    "                 for g in optimizer.param_groups:\n",
    "                     g['lr'] = new_lr\n",
    "\n",
    "\n",
    "        for imgs, labels in tqdm(train_loader, unit='batch', desc=f\"Task {task} Epoch {epoch+1}\"):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net_to_train(imgs)\n",
    "            loss = criterion(outputs, labels) # Loss calculated on combined batch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # validation loss computation\n",
    "        current_val_loss = val_net(net_to_train, val_loader)\n",
    "        val_losses.append(current_val_loss)\n",
    "\n",
    "        print(f\"Task {task}, Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}, Val Loss: {current_val_loss:.4f}\")\n",
    "\n",
    "        # logging\n",
    "        if save_file:\n",
    "             with open(save_file, 'a') as f:\n",
    "                  f.write(f\"{task},{epoch + 1},{avg_loss},{current_val_loss}\\n\")\n",
    "\n",
    "        # early stopping based on validation loss\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "               torch.save(net_to_train.state_dict(), os.path.join(save_path, f\"model_task{task}_best.pth\"))\n",
    "               print(f\"  New best validation loss: {best_val_loss:.4f}. Saved best model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1} for task {task}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            if save_path and os.path.exists(os.path.join(save_path, f\"model_task{task}_best.pth\")):\n",
    "               print(\"Loading best model weights before exiting.\")\n",
    "               net_to_train.load_state_dict(torch.load(os.path.join(save_path, f\"model_task{task}_best.pth\")))\n",
    "            break\n",
    "\n",
    "    print(f\"Finished training task {task}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b168310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def modify_resnet_head(model, num_classes):\n",
    "  \"\"\"\n",
    "  Modify the last fully connected layer of the ResNet model to match the number of classes.\n",
    "  \"\"\"\n",
    "\n",
    "  old_fc = model.fc\n",
    "  old_num_classes = old_fc.out_features\n",
    "  num_ftrs = old_fc.in_features\n",
    "\n",
    "  # Create the new head\n",
    "  new_fc = nn.Linear(num_ftrs, num_classes).cuda()\n",
    "\n",
    "  # Copy weights and biases from the old head\n",
    "  if old_num_classes < num_classes:\n",
    "    new_fc.weight.data[:old_num_classes, :] = old_fc.weight.data.clone().cuda()\n",
    "    new_fc.bias.data[:old_num_classes] = old_fc.bias.data.clone().cuda()\n",
    "\n",
    "  model.fc = new_fc\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca192ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_test_accuracy(model, test_loader, num_classes):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total = 0\n",
    "    correct_per_class = [0] * num_classes\n",
    "    total_per_class = [0] * num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"Testing\", total=len(test_loader)):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            output = model(imgs)\n",
    "            preds = output.argmax(dim=1)\n",
    "\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Per-class stats\n",
    "            for c in range(num_classes):\n",
    "                correct_per_class[c] += ((preds == c) & (labels == c)).sum().item()\n",
    "                total_per_class[c] += (labels == c).sum().item()\n",
    "\n",
    "    overall_acc = correct_preds / total\n",
    "    per_class_acc = [correct_per_class[c] / total_per_class[c] if total_per_class[c] > 0 else 0.0\n",
    "                     for c in range(num_classes)]\n",
    "    return overall_acc, per_class_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1befc",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "save_dir = 'naive'\n",
    "if not os.path.exists(save_dir):\n",
    "  os.makedirs(save_dir)\n",
    "\n",
    "# Initialize the model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"Training on task {task}...\")\n",
    "\n",
    "    model = modify_resnet_head(model, (task+1) * 10)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Get current task dataset\n",
    "    train_loader = DataLoader(\n",
    "         Subset(train_dataset, train_task_idxs[task]),\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    # Get validation and test datasets for current task\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        Subset(val_dataset, val_task_idxs[task]),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Subset(test_dataset, test_task_idxs[task]),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    # initialize the optimizer\n",
    "    if task == 0:\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0015)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4) # Lower LR for full network\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    train_losses, val_losses = train_net(\n",
    "        max_epochs=15,\n",
    "        freeze_epochs=5, # Still relevant if you want staged training\n",
    "        patience=5,\n",
    "        net_to_train=model,\n",
    "        opt=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        task=task,\n",
    "        save_file=os.path.join(save_dir, 'train_val_losses.txt')\n",
    "    )\n",
    "\n",
    "\n",
    "    # evaluate and save the model\n",
    "    overall_acc, per_class_acc = get_test_accuracy(model, test_loader, (task+1) * 10)\n",
    "    print(f\"Overall accuracy for task {task} (on classes 0-{(task+1)*10 - 1}): {overall_acc:.4f}\")\n",
    "\n",
    "    with open(os.path.join(save_dir, 'accuracies.txt'), 'a') as f:\n",
    "        f.write(f\"{task},{overall_acc:.4f},{per_class_acc}\\n\")\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"model_task_{task}.pth\"))\n",
    "    print(f\"Model for task {task} saved as model_task_{task}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
