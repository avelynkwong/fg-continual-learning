{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d3c817",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains the code for visualizing the model results, for analysis and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15766882",
   "metadata": {},
   "source": [
    "### Plotting Accuracy Drops Across Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99268555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def viz_acc_drops(accuracy_file_path, num_total_classes, num_classes_per_task, dataset):\n",
    "    parsed_data = {}\n",
    "    with open(accuracy_file_path, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "\n",
    "        for line_num, line in enumerate(f, start=2):\n",
    "            line = line.strip()\n",
    "            first_comma_idx = line.find(',')\n",
    "            task_id_str = line[:first_comma_idx]\n",
    "            task_id = int(task_id_str)\n",
    "            second_comma_idx = line.find(',', first_comma_idx + 1)\n",
    "            list_str = line[second_comma_idx + 1:]\n",
    "            per_class_acc_list = ast.literal_eval(list_str)\n",
    "            if not isinstance(per_class_acc_list, list):\n",
    "                raise TypeError(f\"Parsed data for 'Per-Class Accuracy' is not a list (type: {type(per_class_acc_list)}).\")\n",
    "            # float conversion\n",
    "            per_class_acc_list = [float(acc) for acc in per_class_acc_list]\n",
    "            parsed_data[task_id] = per_class_acc_list\n",
    "\n",
    "\n",
    "    num_tasks_found = len(parsed_data)\n",
    "    if num_tasks_found == 0:\n",
    "        print(\"No valid task data parsed. Exiting.\")\n",
    "        exit()\n",
    "    \n",
    "    # create a matrix: rows=tasks, cols=classes\n",
    "    acc_matrix = np.full((num_tasks_found, num_total_classes), np.nan)\n",
    "    sorted_task_ids = sorted(parsed_data.keys())\n",
    "\n",
    "    for task_idx, task_id in enumerate(sorted_task_ids):\n",
    "        accuracies = parsed_data[task_id]\n",
    "        num_classes_in_task_eval = len(accuracies)\n",
    "        if num_classes_in_task_eval > num_total_classes:\n",
    "            print(f\"Warning: Task {task_id} reported {num_classes_in_task_eval} accuracies, exceeding total classes {num_total_classes}. Truncating.\")\n",
    "            num_classes_in_task_eval = num_total_classes\n",
    "            accuracies = accuracies[:num_total_classes]\n",
    "        acc_matrix[task_idx, :num_classes_in_task_eval] = accuracies\n",
    "\n",
    "    # plot the heatmap\n",
    "    plt.figure(figsize=(20, max(5, num_tasks_found * 0.7)))\n",
    "\n",
    "    heatmap = sns.heatmap(\n",
    "        acc_matrix,\n",
    "        annot=False,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.2,\n",
    "        linecolor='lightgrey',\n",
    "        cbar_kws={'label': 'Per-Class Accuracy'},\n",
    "        vmin=0.0,\n",
    "        vmax=1.0\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Evaluation Point (After Task X Completed)\")\n",
    "    plt.title(f\"Per-Class Accuracy After Each Task - {dataset}\", fontsize=16)\n",
    "    plt.xticks(ticks=np.arange(0, num_total_classes, 5) + 0.5, labels=np.arange(0, num_total_classes, 5), rotation=90, fontsize=8)\n",
    "    plt.yticks(ticks=np.arange(num_tasks_found) + 0.5, labels=[f\"After Task {t}\" for t in sorted_task_ids], rotation=0)\n",
    "\n",
    "    for i in range(num_classes_per_task, num_total_classes, num_classes_per_task):\n",
    "        plt.axvline(x=i, color='white', linestyle='--', linewidth=1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texture dataset\n",
    "accuracy_file_path = 'accuracies-DTD.txt'\n",
    "num_total_classes = 47\n",
    "num_classes_per_task = 10\n",
    "\n",
    "viz_acc_drops(accuracy_file_path, num_total_classes, num_classes_per_task, 'DTD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ed6cc",
   "metadata": {},
   "source": [
    "### Plotting Relative Forgetting Measure\n",
    "\n",
    "Average Relative Forgetting Measure is the proportion drop in accuracy for each task from the peak, averaged across all seen-before tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using relative forgetting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def viz_relative_forgetting_measure(filepath, dataset):\n",
    "  \"\"\"\n",
    "  Calculates and visualizes the average relative forgetting.\n",
    "\n",
    "  Relative Forgetting for a previous task i after learning task t is:\n",
    "  (max_accuracy(task i) - current_accuracy(task i)) / max_accuracy(task i).\n",
    "  If max_accuracy(task i) is 0, relative forgetting is 0.\n",
    "  Ensures the value is non-negative.\n",
    "\n",
    "  The average relative forgetting after task t is the average of this\n",
    "  value across all tasks i < t.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filepath, 'r') as f:\n",
    "    for line_num, line in enumerate(f, start=2):\n",
    "      line = line.strip()\n",
    "      parts = line.strip().split(',', 2)\n",
    "      if len(parts) == 3:\n",
    "        task = int(parts[0])\n",
    "        overall_acc = float(parts[1])\n",
    "        per_class_str = parts[2]\n",
    "        per_class_list = ast.literal_eval(per_class_str)\n",
    "        data.append({\n",
    "            'Task': task,\n",
    "            'Overall Accuracy': overall_acc,\n",
    "            'Per-Class Accuracy': [float(acc) for acc in per_class_list]\n",
    "        })\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "  df = df.sort_values(by='Task').reset_index(drop=True)\n",
    "  num_tasks = len(df)\n",
    "\n",
    "  # find classes per task\n",
    "  classes_per_task = []\n",
    "  cumulative_classes = 0\n",
    "  class_indices_per_task = {}\n",
    "  for i in range(num_tasks):\n",
    "      current_total_classes = len(df.loc[i, 'Per-Class Accuracy'])\n",
    "      num_new_classes = current_total_classes - cumulative_classes\n",
    "      classes_per_task.append(num_new_classes)\n",
    "      task_class_indices = list(range(cumulative_classes, current_total_classes))\n",
    "      class_indices_per_task[i] = task_class_indices\n",
    "      cumulative_classes = current_total_classes\n",
    "\n",
    "\n",
    "  print(f\"Detected {num_tasks} tasks.\")\n",
    "  print(f\"Classes introduced per task: {classes_per_task}\")\n",
    "\n",
    "  # peak acc per task\n",
    "  peak_task_accuracies = {}\n",
    "  for i in range(num_tasks):\n",
    "    task_class_indices = class_indices_per_task.get(i, [])\n",
    "    accuracies_at_eval_i = df.loc[i, 'Per-Class Accuracy']\n",
    "\n",
    "    # get accuracies for task i's classes\n",
    "    task_i_class_accuracies = [accuracies_at_eval_i[idx] for idx in task_class_indices]\n",
    "    peak_task_accuracies[i] = np.mean(task_i_class_accuracies) # store the average of accuracies for the 10 classes belonging to this task\n",
    "\n",
    "\n",
    "  print(f\"Peak average accuracy per task (Acc(i,i)): {peak_task_accuracies}\")\n",
    "\n",
    "  # get average relative forgetting\n",
    "  average_relative_forgetting = []\n",
    "  evaluation_points = [] # tasks after which forgetting is measured (1 to T-1)\n",
    "\n",
    "  for t in range(1, num_tasks):\n",
    "      total_relative_forgetting_at_t = 0.0\n",
    "      num_previous_tasks_measured = 0\n",
    "\n",
    "      accuracies_at_eval_t = df.loc[t, 'Per-Class Accuracy']\n",
    "\n",
    "      for i in range(t):\n",
    "        task_i_class_indices = class_indices_per_task.get(i, [])\n",
    "\n",
    "        # extract accuracies for task i's classes measured after task t\n",
    "        task_i_accuracies_at_t = [accuracies_at_eval_t[idx] for idx in task_i_class_indices]\n",
    "        current_acc_i_at_t = np.mean(task_i_accuracies_at_t) if task_i_accuracies_at_t else 0.0\n",
    "\n",
    "        peak_acc_i = peak_task_accuracies.get(i, 0)\n",
    "\n",
    "        # relative forgetting calculation\n",
    "        if peak_acc_i > 1e-6:\n",
    "            relative_forgetting_i_at_t = (peak_acc_i - current_acc_i_at_t) / peak_acc_i\n",
    "        else:\n",
    "            relative_forgetting_i_at_t = 0.0\n",
    "        total_relative_forgetting_at_t += relative_forgetting_i_at_t\n",
    "        num_previous_tasks_measured += 1\n",
    "\n",
    "      avg_relative_forgetting_t = total_relative_forgetting_at_t / num_previous_tasks_measured\n",
    "      average_relative_forgetting.append(avg_relative_forgetting_t)\n",
    "      evaluation_points.append(t + 1)\n",
    "\n",
    "  evaluation_points_xaxis = [pt for pt in range(1, num_tasks)] # Should match len(average_relative_forgetting)\n",
    "  print(f\"\\nAverage Relative Forgetting after tasks {evaluation_points_xaxis}: {average_relative_forgetting}\")\n",
    "\n",
    "\n",
    "  # plotting\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  if len(evaluation_points_xaxis) == len(average_relative_forgetting):\n",
    "      plt.plot(evaluation_points_xaxis, average_relative_forgetting, marker='o', linestyle='-')\n",
    "      plt.xlabel(\"Task Number Completed (Evaluation Point)\")\n",
    "      plt.ylabel(\"Average Relative Forgetting\")\n",
    "      plt.title(f\"Average Relative Forgetting vs. Number of Tasks Completed - {dataset}\")\n",
    "      plt.xticks(evaluation_points_xaxis)\n",
    "      plt.grid(True, linestyle='--', alpha=0.6)\n",
    "      plt.ylim(bottom=0, top=1.05)\n",
    "      plt.show()\n",
    "  else:\n",
    "      print(\"Error: Mismatch between evaluation points and calculated forgetting values. Cannot plot.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
