{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d3c817",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains the code for visualizing the model results, for analysis and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15766882",
   "metadata": {},
   "source": [
    "### Plotting Accuracy Drops Across Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99268555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def viz_acc_drops(accuracy_file_path, num_total_classes, num_classes_per_task, dataset):\n",
    "    parsed_data = {}\n",
    "    with open(accuracy_file_path, 'r') as f:\n",
    "        header = f.readline().strip()\n",
    "\n",
    "        for line_num, line in enumerate(f, start=2):\n",
    "            line = line.strip()\n",
    "            first_comma_idx = line.find(',')\n",
    "            task_id_str = line[:first_comma_idx]\n",
    "            task_id = int(task_id_str)\n",
    "            second_comma_idx = line.find(',', first_comma_idx + 1)\n",
    "            list_str = line[second_comma_idx + 1:]\n",
    "            per_class_acc_list = ast.literal_eval(list_str)\n",
    "            if not isinstance(per_class_acc_list, list):\n",
    "                raise TypeError(f\"Parsed data for 'Per-Class Accuracy' is not a list (type: {type(per_class_acc_list)}).\")\n",
    "            # float conversion\n",
    "            per_class_acc_list = [float(acc) for acc in per_class_acc_list]\n",
    "            parsed_data[task_id] = per_class_acc_list\n",
    "\n",
    "\n",
    "    num_tasks_found = len(parsed_data)\n",
    "    if num_tasks_found == 0:\n",
    "        print(\"No valid task data parsed. Exiting.\")\n",
    "        exit()\n",
    "    \n",
    "    # create a matrix: rows=tasks, cols=classes\n",
    "    acc_matrix = np.full((num_tasks_found, num_total_classes), np.nan)\n",
    "    sorted_task_ids = sorted(parsed_data.keys())\n",
    "\n",
    "    for task_idx, task_id in enumerate(sorted_task_ids):\n",
    "        accuracies = parsed_data[task_id]\n",
    "        num_classes_in_task_eval = len(accuracies)\n",
    "        if num_classes_in_task_eval > num_total_classes:\n",
    "            print(f\"Warning: Task {task_id} reported {num_classes_in_task_eval} accuracies, exceeding total classes {num_total_classes}. Truncating.\")\n",
    "            num_classes_in_task_eval = num_total_classes\n",
    "            accuracies = accuracies[:num_total_classes]\n",
    "        acc_matrix[task_idx, :num_classes_in_task_eval] = accuracies\n",
    "\n",
    "    # plot the heatmap\n",
    "    plt.figure(figsize=(20, max(5, num_tasks_found * 0.7)))\n",
    "\n",
    "    heatmap = sns.heatmap(\n",
    "        acc_matrix,\n",
    "        annot=False,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.2,\n",
    "        linecolor='lightgrey',\n",
    "        cbar_kws={'label': 'Per-Class Accuracy'},\n",
    "        vmin=0.0,\n",
    "        vmax=1.0\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Evaluation Point (After Task X Completed)\")\n",
    "    plt.title(f\"Per-Class Accuracy After Each Task - {dataset}\", fontsize=16)\n",
    "    plt.xticks(ticks=np.arange(0, num_total_classes, 5) + 0.5, labels=np.arange(0, num_total_classes, 5), rotation=90, fontsize=8)\n",
    "    plt.yticks(ticks=np.arange(num_tasks_found) + 0.5, labels=[f\"After Task {t}\" for t in sorted_task_ids], rotation=0)\n",
    "\n",
    "    for i in range(num_classes_per_task, num_total_classes, num_classes_per_task):\n",
    "        plt.axvline(x=i, color='white', linestyle='--', linewidth=1.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd4ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texture dataset\n",
    "accuracy_file_path = 'accuracies-DTD.txt'\n",
    "num_total_classes = 47\n",
    "num_classes_per_task = 10\n",
    "\n",
    "viz_acc_drops(accuracy_file_path, num_total_classes, num_classes_per_task, 'DTD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ed6cc",
   "metadata": {},
   "source": [
    "### Plotting Relative Forgetting Measure\n",
    "\n",
    "Average Relative Forgetting Measure is the proportion drop in accuracy for each task from the peak, averaged across all seen-before tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using relative forgetting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def viz_relative_forgetting_measure(filepath, dataset):\n",
    "  \"\"\"\n",
    "  Calculates and visualizes the average relative forgetting.\n",
    "\n",
    "  Relative Forgetting for a previous task i after learning task t is:\n",
    "  (max_accuracy(task i) - current_accuracy(task i)) / max_accuracy(task i).\n",
    "  If max_accuracy(task i) is 0, relative forgetting is 0.\n",
    "  Ensures the value is non-negative.\n",
    "\n",
    "  The average relative forgetting after task t is the average of this\n",
    "  value across all tasks i < t.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  with open(filepath, 'r') as f:\n",
    "    for line_num, line in enumerate(f, start=2):\n",
    "      line = line.strip()\n",
    "      parts = line.strip().split(',', 2)\n",
    "      if len(parts) == 3:\n",
    "        task = int(parts[0])\n",
    "        overall_acc = float(parts[1])\n",
    "        per_class_str = parts[2]\n",
    "        per_class_list = ast.literal_eval(per_class_str)\n",
    "        data.append({\n",
    "            'Task': task,\n",
    "            'Overall Accuracy': overall_acc,\n",
    "            'Per-Class Accuracy': [float(acc) for acc in per_class_list]\n",
    "        })\n",
    "\n",
    "  df = pd.DataFrame(data)\n",
    "  df = df.sort_values(by='Task').reset_index(drop=True)\n",
    "  num_tasks = len(df)\n",
    "\n",
    "  # find classes per task\n",
    "  classes_per_task = []\n",
    "  cumulative_classes = 0\n",
    "  class_indices_per_task = {}\n",
    "  for i in range(num_tasks):\n",
    "      current_total_classes = len(df.loc[i, 'Per-Class Accuracy'])\n",
    "      num_new_classes = current_total_classes - cumulative_classes\n",
    "      classes_per_task.append(num_new_classes)\n",
    "      task_class_indices = list(range(cumulative_classes, current_total_classes))\n",
    "      class_indices_per_task[i] = task_class_indices\n",
    "      cumulative_classes = current_total_classes\n",
    "\n",
    "\n",
    "  print(f\"Detected {num_tasks} tasks.\")\n",
    "  print(f\"Classes introduced per task: {classes_per_task}\")\n",
    "\n",
    "  # peak acc per task\n",
    "  peak_task_accuracies = {}\n",
    "  for i in range(num_tasks):\n",
    "    task_class_indices = class_indices_per_task.get(i, [])\n",
    "    accuracies_at_eval_i = df.loc[i, 'Per-Class Accuracy']\n",
    "\n",
    "    # get accuracies for task i's classes\n",
    "    task_i_class_accuracies = [accuracies_at_eval_i[idx] for idx in task_class_indices]\n",
    "    peak_task_accuracies[i] = np.mean(task_i_class_accuracies) # store the average of accuracies for the 10 classes belonging to this task\n",
    "\n",
    "\n",
    "  print(f\"Peak average accuracy per task (Acc(i,i)): {peak_task_accuracies}\")\n",
    "\n",
    "  # get average relative forgetting\n",
    "  average_relative_forgetting = []\n",
    "  evaluation_points = [] # tasks after which forgetting is measured (1 to T-1)\n",
    "\n",
    "  for t in range(1, num_tasks):\n",
    "      total_relative_forgetting_at_t = 0.0\n",
    "      num_previous_tasks_measured = 0\n",
    "\n",
    "      accuracies_at_eval_t = df.loc[t, 'Per-Class Accuracy']\n",
    "\n",
    "      for i in range(t):\n",
    "        task_i_class_indices = class_indices_per_task.get(i, [])\n",
    "\n",
    "        # extract accuracies for task i's classes measured after task t\n",
    "        task_i_accuracies_at_t = [accuracies_at_eval_t[idx] for idx in task_i_class_indices]\n",
    "        current_acc_i_at_t = np.mean(task_i_accuracies_at_t) if task_i_accuracies_at_t else 0.0\n",
    "\n",
    "        peak_acc_i = peak_task_accuracies.get(i, 0)\n",
    "\n",
    "        # relative forgetting calculation\n",
    "        if peak_acc_i > 1e-6:\n",
    "            relative_forgetting_i_at_t = (peak_acc_i - current_acc_i_at_t) / peak_acc_i\n",
    "        else:\n",
    "            relative_forgetting_i_at_t = 0.0\n",
    "        total_relative_forgetting_at_t += relative_forgetting_i_at_t\n",
    "        num_previous_tasks_measured += 1\n",
    "\n",
    "      avg_relative_forgetting_t = total_relative_forgetting_at_t / num_previous_tasks_measured\n",
    "      average_relative_forgetting.append(avg_relative_forgetting_t)\n",
    "      evaluation_points.append(t + 1)\n",
    "\n",
    "  evaluation_points_xaxis = [pt for pt in range(1, num_tasks)] # Should match len(average_relative_forgetting)\n",
    "  print(f\"\\nAverage Relative Forgetting after tasks {evaluation_points_xaxis}: {average_relative_forgetting}\")\n",
    "\n",
    "\n",
    "  # plotting\n",
    "  plt.figure(figsize=(8, 5))\n",
    "  if len(evaluation_points_xaxis) == len(average_relative_forgetting):\n",
    "      plt.plot(evaluation_points_xaxis, average_relative_forgetting, marker='o', linestyle='-')\n",
    "      plt.xlabel(\"Task Number Completed (Evaluation Point)\")\n",
    "      plt.ylabel(\"Average Relative Forgetting\")\n",
    "      plt.title(f\"Average Relative Forgetting vs. Number of Tasks Completed - {dataset}\")\n",
    "      plt.xticks(evaluation_points_xaxis)\n",
    "      plt.grid(True, linestyle='--', alpha=0.6)\n",
    "      plt.ylim(bottom=0, top=1.05)\n",
    "      plt.show()\n",
    "  else:\n",
    "      print(\"Error: Mismatch between evaluation points and calculated forgetting values. Cannot plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b764d7f",
   "metadata": {},
   "source": [
    "### Confusion Matrix Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# test_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='test',\n",
    "#     annotation_level=granularity,\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c952bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "# seeds\n",
    "seed = 88\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbac69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def modify_resnet_head(model, num_classes):\n",
    "  \"\"\"\n",
    "  Modify the last fully connected layer of the ResNet model to match the number of classes.\n",
    "  \"\"\"\n",
    "\n",
    "  old_fc = model.fc\n",
    "  old_num_classes = old_fc.out_features\n",
    "  num_ftrs = old_fc.in_features\n",
    "\n",
    "  # Create the new head\n",
    "  new_fc = nn.Linear(num_ftrs, num_classes).cuda()\n",
    "\n",
    "  # Copy weights and biases from the old head\n",
    "  if old_num_classes < num_classes:\n",
    "    new_fc.weight.data[:old_num_classes, :] = old_fc.weight.data.clone().cuda()\n",
    "    new_fc.bias.data[:old_num_classes] = old_fc.bias.data.clone().cuda()\n",
    "\n",
    "  model.fc = new_fc\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee266dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(model, test_loader, num_classes):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for a given model and dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            output = model(imgs)\n",
    "            preds = output.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    return cm\n",
    "\n",
    "num_classes = 47\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "modify_resnet_head(model, num_classes)\n",
    "model.load_state_dict(torch.load(\"model-naive-DTD.pth\"))\n",
    "model.cuda()\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# get the confusion matrix\n",
    "cm = get_confusion_matrix(model, test_loader, num_classes)\n",
    "\n",
    "# --- display the Confusion Matrix ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d35a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def calculate_and_visualize_bwt(accuracy_file_paths, dataset_names):\n",
    "    \"\"\"\n",
    "    Calculates Backward Transfer (BWT) from per-class accuracies in multiple text files\n",
    "    and visualizes them with a grouped bar chart.\n",
    "    \"\"\"\n",
    "\n",
    "    num_datasets = len(accuracy_file_paths)\n",
    "    all_bwt_values = []\n",
    "\n",
    "    for file_path in accuracy_file_paths:\n",
    "        parsed_data = {}\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    task_id_str, _, per_class_acc_str = line.split(',', 2)\n",
    "                    task_id = int(task_id_str)\n",
    "                    per_class_acc_list = ast.literal_eval(per_class_acc_str)\n",
    "                    parsed_data[task_id] = [float(acc) for acc in per_class_acc_list]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Accuracy file not found at '{file_path}'\")\n",
    "            return\n",
    "\n",
    "        # calculate BWT\n",
    "        bwt_values = []\n",
    "        for task_id in range(1, len(parsed_data)):\n",
    "            prev_task_accuracies = parsed_data.get(task_id - 1, [])\n",
    "            current_task_accuracies = parsed_data.get(task_id, [])\n",
    "\n",
    "            relevant_classes = len(prev_task_accuracies)\n",
    "            if relevant_classes > 0:\n",
    "                initial_performance = np.mean(prev_task_accuracies)\n",
    "                current_performance = np.mean(current_task_accuracies[:relevant_classes])\n",
    "                bwt = initial_performance - current_performance\n",
    "                bwt_values.append(bwt)\n",
    "\n",
    "        all_bwt_values.append(bwt_values)\n",
    "\n",
    "    # Plot BWT values\n",
    "    num_tasks = len(all_bwt_values[0])\n",
    "    task_labels = [f\"Task {i}\" for i in range(1, num_tasks + 1)]\n",
    "    bar_width = 0.8 / num_datasets\n",
    "    x_positions = np.arange(num_tasks)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        ax.bar(x_positions + i * bar_width, all_bwt_values[i], width=bar_width, label=dataset_name)\n",
    "\n",
    "    ax.set_xlabel(\"Task\")\n",
    "    ax.set_ylabel(\"Backward Transfer (BWT)\")\n",
    "    ax.set_title(\"Backward Transfer Comparison\")\n",
    "    ax.set_xticks(x_positions + bar_width * (num_datasets -1)/2)\n",
    "    ax.set_xticklabels(task_labels, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
