{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10a005b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains the replay-only approach for training coarse-grained and fine-grained datasets in a continual learning classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67295c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds for reproducibility\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# seeds\n",
    "seed = 88\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb8a2b",
   "metadata": {},
   "source": [
    "## Data Loading and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# remove copyright banner\n",
    "class RemoveCopyrightBanner(object):\n",
    "    def __call__(self, img):\n",
    "        width, height = img.size\n",
    "        return img.crop((0, 0, width, height - 20))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    RemoveCopyrightBanner(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# granularity = 'variant'\n",
    "\n",
    "# # Create the FGVC Aircraft dataset instance\n",
    "# train_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='trainval',              # Options: 'train', 'val', 'trainval', 'test'\n",
    "#     annotation_level=granularity,    # Options: 'variant', 'family', 'manufacturer'\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# val_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='val',\n",
    "#     annotation_level='variant',\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "# test_dataset = FGVCAircraft(\n",
    "#     root='./data',\n",
    "#     split='test',\n",
    "#     annotation_level=granularity,\n",
    "#     transform=transform,\n",
    "#     download=True\n",
    "# )\n",
    "\n",
    "data_root = './data'\n",
    "\n",
    "train_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='val',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = datasets.DTD(\n",
    "    root=data_root,\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainval_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [0.8, 0.2], generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a50ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show images\n",
    "def show_images(train_dataset, num_images=5):\n",
    "  #shuffle the dataset\n",
    "  train_dataset = torch.utils.data.Subset(train_dataset, torch.randperm(len(train_dataset)))\n",
    "  fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "  for i in range(num_images):\n",
    "      image, label = train_dataset[i]\n",
    "      image = image.permute(1, 2, 0)  # convert from CxHxW to HxWxC\n",
    "      axes[i].imshow(image)\n",
    "      axes[i].set_title(f'Label: {label}')\n",
    "      axes[i].axis('off')\n",
    "  plt.show()\n",
    "\n",
    "show_images(train_dataset, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85998420",
   "metadata": {},
   "source": [
    "## Create the Dataset\n",
    "In a continual learning setting, each task contains a new set of classes to train the model on. The validation and test datasets should be cumulative (to evalute the performance of the model on all classes seen by the model so far)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def group_task_indices(dataset, cumulative=False, max_per_class=1000, classes_per_task=10):\n",
    "    \"\"\"\n",
    "    Task 0: 0-9, Task 1: 10-19, ..., Task 9: 90-99 for train\n",
    "    Output a dictionary where keys are task indices and values are lists of image indices.\n",
    "    For example, task_dict[0] will contain indices of images with labels 0-9.\n",
    "    \"\"\"\n",
    "    per_class_counts = defaultdict(int)\n",
    "    task_dict = defaultdict(list)\n",
    "    for idx, (_, label) in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "        # for test and val, should have cumulative indices (all classes seen so far)\n",
    "        if per_class_counts[label] >= max_per_class:\n",
    "          continue\n",
    "        per_class_counts[label] += 1\n",
    "        if cumulative:\n",
    "          for i in range((label // classes_per_task), classes_per_task):\n",
    "            task_dict[i].append(idx)\n",
    "        else:\n",
    "          task_dict[label // classes_per_task].append(idx)\n",
    "    return task_dict\n",
    "\n",
    "train_task_idxs = group_task_indices(train_dataset, cumulative=False, max_per_class=60)\n",
    "val_task_idxs = group_task_indices(val_dataset, cumulative=True)\n",
    "test_task_idxs = group_task_indices(test_dataset, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b183930",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_net(net_to_val, val_loader):\n",
    "    net_to_val.eval()\n",
    "    loss = 0\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(val_loader, desc=\"Validating\"):\n",
    "\n",
    "            # Get the input images and their corresponding labels\n",
    "            img, label = img.cuda(), label.cuda()\n",
    "\n",
    "            # Forward pass: Get predictions from the model\n",
    "            outputs = net_to_val(img)\n",
    "            loss += criterion(outputs, label)\n",
    "\n",
    "        return loss / len(val_loader)\n",
    "\n",
    "def train_net(max_epochs, freeze_epochs, patience, net_to_train, opt, train_loader, val_loader, task, save_file=None, save_path=None):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    net_to_train.cuda()\n",
    "\n",
    "    initial_freeze = (task == 0) # only freeze backbone for task 0\n",
    "\n",
    "    for name, param in net_to_train.named_parameters():\n",
    "        # do not train non-fc layers\n",
    "        if initial_freeze and 'fc' not in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    optimizer = opt\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    print(f\"Starting training for Task {task}. Trainable parameters:\")\n",
    "    for name, param in net_to_train.named_parameters():\n",
    "        if param.requires_grad:\n",
    "             print(f\"  - {name}\")\n",
    "\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        net_to_train.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Unfreeze logic (if desired for staged training within a task)\n",
    "        if epoch == freeze_epochs and task > 0: # Only unfreeze/adjust LR if NOT task 0 and freeze_epochs > 0\n",
    "            print(f\"Unfreezing backbone at epoch {epoch} for task {task}\")\n",
    "            for param in net_to_train.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Adjust LR for the existing optimizer\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            new_lr = 1e-4\n",
    "            if current_lr != new_lr:\n",
    "                 print(f\"Setting LR to {new_lr}\")\n",
    "                 for g in optimizer.param_groups:\n",
    "                     g['lr'] = new_lr\n",
    "\n",
    "\n",
    "        for imgs, labels in tqdm(train_loader, unit='batch', desc=f\"Task {task} Epoch {epoch+1}\"):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net_to_train(imgs)\n",
    "            loss = criterion(outputs, labels) # Loss calculated on combined batch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # validation loss computation\n",
    "        current_val_loss = val_net(net_to_train, val_loader)\n",
    "        val_losses.append(current_val_loss)\n",
    "\n",
    "        print(f\"Task {task}, Epoch {epoch + 1}, Train Loss: {avg_loss:.4f}, Val Loss: {current_val_loss:.4f}\")\n",
    "\n",
    "        # logging\n",
    "        if save_file:\n",
    "             with open(save_file, 'a') as f:\n",
    "                  f.write(f\"{task},{epoch + 1},{avg_loss},{current_val_loss}\\n\")\n",
    "\n",
    "        # early stopping based on validation loss\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "               torch.save(net_to_train.state_dict(), os.path.join(save_path, f\"model_task{task}_best.pth\"))\n",
    "               print(f\"  New best validation loss: {best_val_loss:.4f}. Saved best model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1} for task {task}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "            if save_path and os.path.exists(os.path.join(save_path, f\"model_task{task}_best.pth\")):\n",
    "               print(\"Loading best model weights before exiting.\")\n",
    "               net_to_train.load_state_dict(torch.load(os.path.join(save_path, f\"model_task{task}_best.pth\")))\n",
    "            break\n",
    "\n",
    "    print(f\"Finished training task {task}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b168310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def modify_resnet_head(model, num_classes):\n",
    "  \"\"\"\n",
    "  Modify the last fully connected layer of the ResNet model to match the number of classes.\n",
    "  \"\"\"\n",
    "\n",
    "  old_fc = model.fc\n",
    "  old_num_classes = old_fc.out_features\n",
    "  num_ftrs = old_fc.in_features\n",
    "\n",
    "  # Create the new head\n",
    "  new_fc = nn.Linear(num_ftrs, num_classes).cuda()\n",
    "\n",
    "  # Copy weights and biases from the old head\n",
    "  if old_num_classes < num_classes:\n",
    "    new_fc.weight.data[:old_num_classes, :] = old_fc.weight.data.clone().cuda()\n",
    "    new_fc.bias.data[:old_num_classes] = old_fc.bias.data.clone().cuda()\n",
    "\n",
    "  model.fc = new_fc\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca192ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_test_accuracy(model, test_loader, num_classes):\n",
    "    model.eval()\n",
    "    correct_preds = 0\n",
    "    total = 0\n",
    "    correct_per_class = [0] * num_classes\n",
    "    total_per_class = [0] * num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=\"Testing\", total=len(test_loader)):\n",
    "            imgs, labels = imgs.cuda(), labels.cuda()\n",
    "            output = model(imgs)\n",
    "            preds = output.argmax(dim=1)\n",
    "\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Per-class stats\n",
    "            for c in range(num_classes):\n",
    "                correct_per_class[c] += ((preds == c) & (labels == c)).sum().item()\n",
    "                total_per_class[c] += (labels == c).sum().item()\n",
    "\n",
    "    overall_acc = correct_preds / total\n",
    "    per_class_acc = [correct_per_class[c] / total_per_class[c] if total_per_class[c] > 0 else 0.0\n",
    "                     for c in range(num_classes)]\n",
    "    return overall_acc, per_class_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd9fd5",
   "metadata": {},
   "source": [
    "## Replay Classes and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce15ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory_buffer(buffer, max_size, new_samples):\n",
    "    \"\"\"Adds new samples to the buffer and trims it if it exceeds max_size.\"\"\"\n",
    "    buffer.extend(new_samples)\n",
    "    # If buffer exceeds max size, remove samples randomly\n",
    "    overflow = len(buffer) - max_size\n",
    "    if overflow > 0:\n",
    "        indices_to_remove = random.sample(range(len(buffer)), overflow)\n",
    "        for index in sorted(indices_to_remove, reverse=True):\n",
    "            del buffer[index]\n",
    "    print(f\"Memory buffer size: {len(buffer)} / {max_size}\")\n",
    "\n",
    "class MemoryDataset(Dataset):\n",
    "    \"\"\"Dataset class for the memory buffer.\"\"\"\n",
    "    def __init__(self, buffer_list):\n",
    "        self.buffer = buffer_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # buffer contains (image_tensor, label)\n",
    "        return self.buffer[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1befc",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846160b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# model initialization\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# memory buffer initialization\n",
    "memory_buffer = [] # store (image_tensor, label) tuples\n",
    "memory_size = 1000 # max size of memory buffer\n",
    "samples_per_task_in_memory = 20 # number of samples to keep in memory for each task\n",
    "\n",
    "save_dir = 'replay-coarse-grained' # Changed directory name\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "open(os.path.join(save_dir, 'train_val_losses.txt'), 'w').close()\n",
    "open(os.path.join(save_dir, 'accuracies.txt'), 'w').write(\"Task,Overall Accuracy,Per-Class Accuracy\\n\")\n",
    "\n",
    "\n",
    "for task in range(5):\n",
    "    print(f\"Training on task {task}...\")\n",
    "\n",
    "    model = modify_resnet_head(model, (task+1) * 10)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # get current task data\n",
    "    current_task_train_subset = Subset(train_dataset, train_task_idxs[task])\n",
    "\n",
    "    # combine with memory buffer if task > 0\n",
    "    if task > 0 and len(memory_buffer) > 0:\n",
    "        replay_dataset = MemoryDataset(memory_buffer)\n",
    "        combined_train_dataset = ConcatDataset([current_task_train_subset, replay_dataset])\n",
    "        print(f\"Task {task}: Training with {len(current_task_train_subset)} current samples and {len(replay_dataset)} replay samples.\")\n",
    "    else:\n",
    "        # task 0 or empty buffer: train only on current task data\n",
    "        combined_train_dataset = current_task_train_subset\n",
    "        print(f\"Task {task}: Training only with {len(current_task_train_subset)} current samples.\")\n",
    "\n",
    "    # dataloader for current task\n",
    "    train_loader_combined = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    # validation and test loaders for the current task\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        Subset(val_dataset, val_task_idxs[task]),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Subset(test_dataset, test_task_idxs[task]),\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    # optimizer initialization\n",
    "    if task == 0:\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0015)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4) # Lower LR for full network\n",
    "\n",
    "\n",
    "    # train the model\n",
    "    train_losses, val_losses = train_net(\n",
    "        max_epochs=15,\n",
    "        freeze_epochs=5,\n",
    "        patience=5,\n",
    "        net_to_train=model,\n",
    "        opt=optimizer,\n",
    "        train_loader=train_loader_combined,\n",
    "        val_loader=val_loader,\n",
    "        task=task,\n",
    "        save_file=os.path.join(save_dir, 'train_val_losses.txt')\n",
    "    )\n",
    "\n",
    "    # select samples from current task for memory buffer\n",
    "    num_to_sample = min(samples_per_task_in_memory, len(current_task_train_subset))\n",
    "    if num_to_sample > 0:\n",
    "        indices_to_sample = random.sample(range(len(current_task_train_subset)), num_to_sample)\n",
    "        new_memory_samples = []\n",
    "        print(f\"Sampling {num_to_sample} examples from task {task} for memory buffer...\")\n",
    "        for idx in indices_to_sample:\n",
    "            img_tensor, label = current_task_train_subset[idx]\n",
    "            new_memory_samples.append((img_tensor, label)) # append as tuple\n",
    "\n",
    "        # add to buffer\n",
    "        update_memory_buffer(memory_buffer, memory_size, new_memory_samples)\n",
    "    else:\n",
    "        print(f\"Not enough samples in task {task} subset to add to memory.\")\n",
    "\n",
    "\n",
    "    # evaluate and save\n",
    "    overall_acc, per_class_acc = get_test_accuracy(model, test_loader, (task+1) * 10)\n",
    "    print(f\"Overall accuracy for task {task} (on classes 0-{(task+1)*10 - 1}): {overall_acc:.4f}\")\n",
    "\n",
    "    with open(os.path.join(save_dir, 'accuracies.txt'), 'a') as f:\n",
    "        f.write(f\"{task},{overall_acc:.4f},{per_class_acc}\\n\")\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"model_task_{task}.pth\"))\n",
    "    print(f\"Model for task {task} saved as model_task_{task}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
