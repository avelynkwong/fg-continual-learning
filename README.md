# Catastrophic Forgetting in Continual Learning on Fine-Grained Datasets

Catastrophic forgetting poses a major challenge for continual learning, especially in computer vision tasks where models must adapt to evolving data streams. While prior research has largely focused on coarse-grained datasets with visually distinct classes, real-world applications often involve = fine-grained domains where class boundaries are subtle. In this work, we systematically evaluate catastrophic forgetting and mitigation strategies-specifically replay and Elastic Weight Consolidation (EWC)-across both coarse-grained (Describable Textures Dataset) and fine-grained (FGVC-Aircraft) settings. Our results reveal that continual learning methods are more effective at reducing forgetting in fine-grained domains as new tasks are introduced. This may be attributed to the shared structural features among similar classes, which allow replay and EWC to better preserve discriminative cues. The combination of replay and EWC proves particularly effective, yielding substantial improvements in information retention for both datasets, but especially within fine-grained tasks. These findings challenge prevailing views on the difficulty of fine-grained continual learning and highlight how class similarities can influence the effectiveness of forgetting mitigation strategies.

This repository contains the code for training and visualizing results obtained from our experiments. 